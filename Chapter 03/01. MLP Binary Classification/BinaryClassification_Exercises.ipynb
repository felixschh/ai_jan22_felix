{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "hazardous-weather",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-26T07:47:37.551793Z",
     "start_time": "2021-05-26T07:47:35.439944Z"
    }
   },
   "outputs": [],
   "source": [
    "import torch as T\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from turtle import forward\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "clean-racing",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-26T07:50:32.482086Z",
     "start_time": "2021-05-26T07:50:32.456893Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 | loss: 0.7246877551078796 \n",
      "Epoch: 2 | loss: 0.7241837382316589 \n",
      "Epoch: 3 | loss: 0.7236850261688232 \n",
      "Epoch: 4 | loss: 0.723191499710083 \n",
      "Epoch: 5 | loss: 0.7227033972740173 \n",
      "Epoch: 6 | loss: 0.7222206592559814 \n",
      "Epoch: 7 | loss: 0.7217434048652649 \n",
      "Epoch: 8 | loss: 0.7212715744972229 \n",
      "Epoch: 9 | loss: 0.7208054065704346 \n",
      "Epoch: 10 | loss: 0.7203447818756104 \n",
      "Epoch: 11 | loss: 0.7198897004127502 \n",
      "Epoch: 12 | loss: 0.7194402813911438 \n",
      "Epoch: 13 | loss: 0.7189965844154358 \n",
      "Epoch: 14 | loss: 0.7185584306716919 \n",
      "Epoch: 15 | loss: 0.7181259989738464 \n",
      "Epoch: 16 | loss: 0.7176991105079651 \n",
      "Epoch: 17 | loss: 0.717278003692627 \n",
      "Epoch: 18 | loss: 0.7168624401092529 \n",
      "Epoch: 19 | loss: 0.7164525389671326 \n",
      "Epoch: 20 | loss: 0.7160483002662659 \n",
      "Epoch: 21 | loss: 0.7156494855880737 \n",
      "Epoch: 22 | loss: 0.7152563333511353 \n",
      "Epoch: 23 | loss: 0.7148687839508057 \n",
      "Epoch: 24 | loss: 0.7144865393638611 \n",
      "Epoch: 25 | loss: 0.7141098976135254 \n",
      "Epoch: 26 | loss: 0.7137385606765747 \n",
      "Epoch: 27 | loss: 0.7133726477622986 \n",
      "Epoch: 28 | loss: 0.7130119204521179 \n",
      "Epoch: 29 | loss: 0.712656557559967 \n",
      "Epoch: 30 | loss: 0.7123063802719116 \n",
      "Epoch: 31 | loss: 0.7119612097740173 \n",
      "Epoch: 32 | loss: 0.7116212248802185 \n",
      "Epoch: 33 | loss: 0.7112863063812256 \n",
      "Epoch: 34 | loss: 0.7109562158584595 \n",
      "Epoch: 35 | loss: 0.7106311321258545 \n",
      "Epoch: 36 | loss: 0.710310697555542 \n",
      "Epoch: 37 | loss: 0.7099952101707458 \n",
      "Epoch: 38 | loss: 0.7096843719482422 \n",
      "Epoch: 39 | loss: 0.7093780636787415 \n",
      "Epoch: 40 | loss: 0.7090764045715332 \n",
      "Epoch: 41 | loss: 0.7087792754173279 \n",
      "Epoch: 42 | loss: 0.7084865570068359 \n",
      "Epoch: 43 | loss: 0.7081981897354126 \n",
      "Epoch: 44 | loss: 0.7079141139984131 \n",
      "Epoch: 45 | loss: 0.7076343297958374 \n",
      "Epoch: 46 | loss: 0.7073588371276855 \n",
      "Epoch: 47 | loss: 0.7070873379707336 \n",
      "Epoch: 48 | loss: 0.706820011138916 \n",
      "Epoch: 49 | loss: 0.7065566182136536 \n",
      "Epoch: 50 | loss: 0.7062971591949463 \n",
      "Epoch: 51 | loss: 0.706041693687439 \n",
      "Epoch: 52 | loss: 0.7057900428771973 \n",
      "Epoch: 53 | loss: 0.7055423259735107 \n",
      "Epoch: 54 | loss: 0.7052983045578003 \n",
      "Epoch: 55 | loss: 0.7050579190254211 \n",
      "Epoch: 56 | loss: 0.7048211693763733 \n",
      "Epoch: 57 | loss: 0.7045881152153015 \n",
      "Epoch: 58 | loss: 0.7043585777282715 \n",
      "Epoch: 59 | loss: 0.7041325569152832 \n",
      "Epoch: 60 | loss: 0.7039099335670471 \n",
      "Epoch: 61 | loss: 0.7036908864974976 \n",
      "Epoch: 62 | loss: 0.7034750580787659 \n",
      "Epoch: 63 | loss: 0.7032626867294312 \n",
      "Epoch: 64 | loss: 0.7030535936355591 \n",
      "Epoch: 65 | loss: 0.7028477191925049 \n",
      "Epoch: 66 | loss: 0.7026451230049133 \n",
      "Epoch: 67 | loss: 0.7024456858634949 \n",
      "Epoch: 68 | loss: 0.7022493481636047 \n",
      "Epoch: 69 | loss: 0.7020561695098877 \n",
      "Epoch: 70 | loss: 0.701866090297699 \n",
      "Epoch: 71 | loss: 0.701678991317749 \n",
      "Epoch: 72 | loss: 0.7014949917793274 \n",
      "Epoch: 73 | loss: 0.701313853263855 \n",
      "Epoch: 74 | loss: 0.7011356353759766 \n",
      "Epoch: 75 | loss: 0.7009603977203369 \n",
      "Epoch: 76 | loss: 0.7007879614830017 \n",
      "Epoch: 77 | loss: 0.7006183862686157 \n",
      "Epoch: 78 | loss: 0.7004516124725342 \n",
      "Epoch: 79 | loss: 0.7002874612808228 \n",
      "Epoch: 80 | loss: 0.7001262903213501 \n",
      "Epoch: 81 | loss: 0.6999677419662476 \n",
      "Epoch: 82 | loss: 0.6998117566108704 \n",
      "Epoch: 83 | loss: 0.6996585130691528 \n",
      "Epoch: 84 | loss: 0.6995078325271606 \n",
      "Epoch: 85 | loss: 0.6993597149848938 \n",
      "Epoch: 86 | loss: 0.6992141604423523 \n",
      "Epoch: 87 | loss: 0.6990711688995361 \n",
      "Epoch: 88 | loss: 0.6989306807518005 \n",
      "Epoch: 89 | loss: 0.698792576789856 \n",
      "Epoch: 90 | loss: 0.6986569762229919 \n",
      "Epoch: 91 | loss: 0.6985237002372742 \n",
      "Epoch: 92 | loss: 0.6983928680419922 \n",
      "Epoch: 93 | loss: 0.6982643604278564 \n",
      "Epoch: 94 | loss: 0.6981382966041565 \n",
      "Epoch: 95 | loss: 0.6980143785476685 \n",
      "Epoch: 96 | loss: 0.6978927850723267 \n",
      "Epoch: 97 | loss: 0.6977733373641968 \n",
      "Epoch: 98 | loss: 0.6976562738418579 \n",
      "Epoch: 99 | loss: 0.697541356086731 \n",
      "Epoch: 100 | loss: 0.6974285244941711 \n",
      "Epoch: 101 | loss: 0.697317898273468 \n",
      "Epoch: 102 | loss: 0.6972092986106873 \n",
      "Epoch: 103 | loss: 0.6971027851104736 \n",
      "Epoch: 104 | loss: 0.6969984173774719 \n",
      "Epoch: 105 | loss: 0.6968960762023926 \n",
      "Epoch: 106 | loss: 0.6967957019805908 \n",
      "Epoch: 107 | loss: 0.6966972947120667 \n",
      "Epoch: 108 | loss: 0.6966008543968201 \n",
      "Epoch: 109 | loss: 0.6965063214302063 \n",
      "Epoch: 110 | loss: 0.6964138150215149 \n",
      "Epoch: 111 | loss: 0.6963230967521667 \n",
      "Epoch: 112 | loss: 0.6962341070175171 \n",
      "Epoch: 113 | loss: 0.696147084236145 \n",
      "Epoch: 114 | loss: 0.6960618495941162 \n",
      "Epoch: 115 | loss: 0.6959784030914307 \n",
      "Epoch: 116 | loss: 0.6958967447280884 \n",
      "Epoch: 117 | loss: 0.6958166360855103 \n",
      "Epoch: 118 | loss: 0.6957384347915649 \n",
      "Epoch: 119 | loss: 0.6956618428230286 \n",
      "Epoch: 120 | loss: 0.6955868601799011 \n",
      "Epoch: 121 | loss: 0.6955135464668274 \n",
      "Epoch: 122 | loss: 0.6954418420791626 \n",
      "Epoch: 123 | loss: 0.695371687412262 \n",
      "Epoch: 124 | loss: 0.695303201675415 \n",
      "Epoch: 125 | loss: 0.6952361464500427 \n",
      "Epoch: 126 | loss: 0.6951705813407898 \n",
      "Epoch: 127 | loss: 0.695106565952301 \n",
      "Epoch: 128 | loss: 0.6950440406799316 \n",
      "Epoch: 129 | loss: 0.6949828267097473 \n",
      "Epoch: 130 | loss: 0.6949231624603271 \n",
      "Epoch: 131 | loss: 0.694864809513092 \n",
      "Epoch: 132 | loss: 0.6948078870773315 \n",
      "Epoch: 133 | loss: 0.6947522759437561 \n",
      "Epoch: 134 | loss: 0.6946979761123657 \n",
      "Epoch: 135 | loss: 0.6946449279785156 \n",
      "Epoch: 136 | loss: 0.6945931911468506 \n",
      "Epoch: 137 | loss: 0.6945427656173706 \n",
      "Epoch: 138 | loss: 0.6944934725761414 \n",
      "Epoch: 139 | loss: 0.6944454908370972 \n",
      "Epoch: 140 | loss: 0.6943986415863037 \n",
      "Epoch: 141 | loss: 0.694352924823761 \n",
      "Epoch: 142 | loss: 0.6943084001541138 \n",
      "Epoch: 143 | loss: 0.6942650079727173 \n",
      "Epoch: 144 | loss: 0.6942225694656372 \n",
      "Epoch: 145 | loss: 0.6941813826560974 \n",
      "Epoch: 146 | loss: 0.694141149520874 \n",
      "Epoch: 147 | loss: 0.6941020488739014 \n",
      "Epoch: 148 | loss: 0.6940637826919556 \n",
      "Epoch: 149 | loss: 0.6940265893936157 \n",
      "Epoch: 150 | loss: 0.6939903497695923 \n",
      "Epoch: 151 | loss: 0.6939550638198853 \n",
      "Epoch: 152 | loss: 0.6939207315444946 \n",
      "Epoch: 153 | loss: 0.6938873529434204 \n",
      "Epoch: 154 | loss: 0.693854808807373 \n",
      "Epoch: 155 | loss: 0.6938230991363525 \n",
      "Epoch: 156 | loss: 0.6937922835350037 \n",
      "Epoch: 157 | loss: 0.6937621831893921 \n",
      "Epoch: 158 | loss: 0.6937329769134521 \n",
      "Epoch: 159 | loss: 0.6937046051025391 \n",
      "Epoch: 160 | loss: 0.6936768293380737 \n",
      "Epoch: 161 | loss: 0.6936500072479248 \n",
      "Epoch: 162 | loss: 0.6936237812042236 \n",
      "Epoch: 163 | loss: 0.6935983300209045 \n",
      "Epoch: 164 | loss: 0.693573534488678 \n",
      "Epoch: 165 | loss: 0.6935495138168335 \n",
      "Epoch: 166 | loss: 0.693526029586792 \n",
      "Epoch: 167 | loss: 0.693503201007843 \n",
      "Epoch: 168 | loss: 0.6934810876846313 \n",
      "Epoch: 169 | loss: 0.6934594511985779 \n",
      "Epoch: 170 | loss: 0.6934384703636169 \n",
      "Epoch: 171 | loss: 0.693418025970459 \n",
      "Epoch: 172 | loss: 0.6933982372283936 \n",
      "Epoch: 173 | loss: 0.6933789253234863 \n",
      "Epoch: 174 | loss: 0.6933601498603821 \n",
      "Epoch: 175 | loss: 0.693341851234436 \n",
      "Epoch: 176 | loss: 0.6933240294456482 \n",
      "Epoch: 177 | loss: 0.6933068037033081 \n",
      "Epoch: 178 | loss: 0.6932899355888367 \n",
      "Epoch: 179 | loss: 0.6932735443115234 \n",
      "Epoch: 180 | loss: 0.6932576894760132 \n",
      "Epoch: 181 | loss: 0.6932421326637268 \n",
      "Epoch: 182 | loss: 0.6932270526885986 \n",
      "Epoch: 183 | loss: 0.6932123303413391 \n",
      "Epoch: 184 | loss: 0.693198025226593 \n",
      "Epoch: 185 | loss: 0.6931840777397156 \n",
      "Epoch: 186 | loss: 0.6931704878807068 \n",
      "Epoch: 187 | loss: 0.6931571960449219 \n",
      "Epoch: 188 | loss: 0.6931443214416504 \n",
      "Epoch: 189 | loss: 0.693131685256958 \n",
      "Epoch: 190 | loss: 0.6931194067001343 \n",
      "Epoch: 191 | loss: 0.6931074261665344 \n",
      "Epoch: 192 | loss: 0.6930958032608032 \n",
      "Epoch: 193 | loss: 0.6930844187736511 \n",
      "Epoch: 194 | loss: 0.6930732727050781 \n",
      "Epoch: 195 | loss: 0.6930623650550842 \n",
      "Epoch: 196 | loss: 0.6930517554283142 \n",
      "Epoch: 197 | loss: 0.6930413842201233 \n",
      "Epoch: 198 | loss: 0.6930311322212219 \n",
      "Epoch: 199 | loss: 0.6930212378501892 \n",
      "Epoch: 200 | loss: 0.6930114030838013 \n",
      "Epoch: 201 | loss: 0.6930019855499268 \n",
      "Epoch: 202 | loss: 0.692992627620697 \n",
      "Epoch: 203 | loss: 0.6929835677146912 \n",
      "Epoch: 204 | loss: 0.6929745674133301 \n",
      "Epoch: 205 | loss: 0.6929656863212585 \n",
      "Epoch: 206 | loss: 0.6929569840431213 \n",
      "Epoch: 207 | loss: 0.6929485201835632 \n",
      "Epoch: 208 | loss: 0.6929401159286499 \n",
      "Epoch: 209 | loss: 0.6929319500923157 \n",
      "Epoch: 210 | loss: 0.692923903465271 \n",
      "Epoch: 211 | loss: 0.6929158568382263 \n",
      "Epoch: 212 | loss: 0.6929081082344055 \n",
      "Epoch: 213 | loss: 0.6929003000259399 \n",
      "Epoch: 214 | loss: 0.6928926706314087 \n",
      "Epoch: 215 | loss: 0.692885160446167 \n",
      "Epoch: 216 | loss: 0.6928776502609253 \n",
      "Epoch: 217 | loss: 0.6928702592849731 \n",
      "Epoch: 218 | loss: 0.6928631067276001 \n",
      "Epoch: 219 | loss: 0.6928558945655823 \n",
      "Epoch: 220 | loss: 0.6928486824035645 \n",
      "Epoch: 221 | loss: 0.692841649055481 \n",
      "Epoch: 222 | loss: 0.6928346157073975 \n",
      "Epoch: 223 | loss: 0.6928277611732483 \n",
      "Epoch: 224 | loss: 0.6928207278251648 \n",
      "Epoch: 225 | loss: 0.6928139328956604 \n",
      "Epoch: 226 | loss: 0.6928070783615112 \n",
      "Epoch: 227 | loss: 0.6928003430366516 \n",
      "Epoch: 228 | loss: 0.692793607711792 \n",
      "Epoch: 229 | loss: 0.6927868127822876 \n",
      "Epoch: 230 | loss: 0.692780077457428 \n",
      "Epoch: 231 | loss: 0.6927734613418579 \n",
      "Epoch: 232 | loss: 0.6927667856216431 \n",
      "Epoch: 233 | loss: 0.6927599906921387 \n",
      "Epoch: 234 | loss: 0.6927534341812134 \n",
      "Epoch: 235 | loss: 0.6927467584609985 \n",
      "Epoch: 236 | loss: 0.6927402019500732 \n",
      "Epoch: 237 | loss: 0.6927334666252136 \n",
      "Epoch: 238 | loss: 0.6927268505096436 \n",
      "Epoch: 239 | loss: 0.6927201151847839 \n",
      "Epoch: 240 | loss: 0.6927134990692139 \n",
      "Epoch: 241 | loss: 0.6927067637443542 \n",
      "Epoch: 242 | loss: 0.6927001476287842 \n",
      "Epoch: 243 | loss: 0.6926933526992798 \n",
      "Epoch: 244 | loss: 0.6926865577697754 \n",
      "Epoch: 245 | loss: 0.6926798224449158 \n",
      "Epoch: 246 | loss: 0.6926730275154114 \n",
      "Epoch: 247 | loss: 0.6926661729812622 \n",
      "Epoch: 248 | loss: 0.692659318447113 \n",
      "Epoch: 249 | loss: 0.6926522850990295 \n",
      "Epoch: 250 | loss: 0.6926453113555908 \n",
      "Epoch: 251 | loss: 0.6926383972167969 \n",
      "Epoch: 252 | loss: 0.6926313638687134 \n",
      "Epoch: 253 | loss: 0.6926242709159851 \n",
      "Epoch: 254 | loss: 0.6926170587539673 \n",
      "Epoch: 255 | loss: 0.6926098465919495 \n",
      "Epoch: 256 | loss: 0.6926026344299316 \n",
      "Epoch: 257 | loss: 0.6925953030586243 \n",
      "Epoch: 258 | loss: 0.6925879716873169 \n",
      "Epoch: 259 | loss: 0.6925804615020752 \n",
      "Epoch: 260 | loss: 0.692573070526123 \n",
      "Epoch: 261 | loss: 0.6925655603408813 \n",
      "Epoch: 262 | loss: 0.6925579905509949 \n",
      "Epoch: 263 | loss: 0.6925501823425293 \n",
      "Epoch: 264 | loss: 0.6925424337387085 \n",
      "Epoch: 265 | loss: 0.6925345659255981 \n",
      "Epoch: 266 | loss: 0.692526638507843 \n",
      "Epoch: 267 | loss: 0.6925187110900879 \n",
      "Epoch: 268 | loss: 0.6925106048583984 \n",
      "Epoch: 269 | loss: 0.6925024390220642 \n",
      "Epoch: 270 | loss: 0.6924942135810852 \n",
      "Epoch: 271 | loss: 0.6924858689308167 \n",
      "Epoch: 272 | loss: 0.6924774050712585 \n",
      "Epoch: 273 | loss: 0.6924689412117004 \n",
      "Epoch: 274 | loss: 0.6924604177474976 \n",
      "Epoch: 275 | loss: 0.6924517750740051 \n",
      "Epoch: 276 | loss: 0.6924429535865784 \n",
      "Epoch: 277 | loss: 0.6924340724945068 \n",
      "Epoch: 278 | loss: 0.6924250721931458 \n",
      "Epoch: 279 | loss: 0.6924160718917847 \n",
      "Epoch: 280 | loss: 0.6924068331718445 \n",
      "Epoch: 281 | loss: 0.6923975348472595 \n",
      "Epoch: 282 | loss: 0.6923881769180298 \n",
      "Epoch: 283 | loss: 0.6923786401748657 \n",
      "Epoch: 284 | loss: 0.6923689842224121 \n",
      "Epoch: 285 | loss: 0.6923593282699585 \n",
      "Epoch: 286 | loss: 0.6923494935035706 \n",
      "Epoch: 287 | loss: 0.6923394799232483 \n",
      "Epoch: 288 | loss: 0.6923294067382812 \n",
      "Epoch: 289 | loss: 0.6923192739486694 \n",
      "Epoch: 290 | loss: 0.6923089623451233 \n",
      "Epoch: 291 | loss: 0.692298412322998 \n",
      "Epoch: 292 | loss: 0.6922879219055176 \n",
      "Epoch: 293 | loss: 0.6922771334648132 \n",
      "Epoch: 294 | loss: 0.6922664046287537 \n",
      "Epoch: 295 | loss: 0.6922553181648254 \n",
      "Epoch: 296 | loss: 0.692244291305542 \n",
      "Epoch: 297 | loss: 0.6922329664230347 \n",
      "Epoch: 298 | loss: 0.6922217011451721 \n",
      "Epoch: 299 | loss: 0.6922100782394409 \n",
      "Epoch: 300 | loss: 0.6921983957290649 \n",
      "Epoch: 301 | loss: 0.6921865940093994 \n",
      "Epoch: 302 | loss: 0.6921746134757996 \n",
      "Epoch: 303 | loss: 0.6921623945236206 \n",
      "Epoch: 304 | loss: 0.6921501755714417 \n",
      "Epoch: 305 | loss: 0.6921377778053284 \n",
      "Epoch: 306 | loss: 0.6921250224113464 \n",
      "Epoch: 307 | loss: 0.6921123266220093 \n",
      "Epoch: 308 | loss: 0.692099392414093 \n",
      "Epoch: 309 | loss: 0.6920861601829529 \n",
      "Epoch: 310 | loss: 0.6920729875564575 \n",
      "Epoch: 311 | loss: 0.6920595765113831 \n",
      "Epoch: 312 | loss: 0.6920459270477295 \n",
      "Epoch: 313 | loss: 0.6920321583747864 \n",
      "Epoch: 314 | loss: 0.6920181512832642 \n",
      "Epoch: 315 | loss: 0.6920039653778076 \n",
      "Epoch: 316 | loss: 0.6919896006584167 \n",
      "Epoch: 317 | loss: 0.6919751167297363 \n",
      "Epoch: 318 | loss: 0.6919602751731873 \n",
      "Epoch: 319 | loss: 0.6919453144073486 \n",
      "Epoch: 320 | loss: 0.6919301748275757 \n",
      "Epoch: 321 | loss: 0.6919147968292236 \n",
      "Epoch: 322 | loss: 0.6918992400169373 \n",
      "Epoch: 323 | loss: 0.691883385181427 \n",
      "Epoch: 324 | loss: 0.6918675303459167 \n",
      "Epoch: 325 | loss: 0.6918512582778931 \n",
      "Epoch: 326 | loss: 0.6918348670005798 \n",
      "Epoch: 327 | loss: 0.6918182373046875 \n",
      "Epoch: 328 | loss: 0.6918013691902161 \n",
      "Epoch: 329 | loss: 0.6917842626571655 \n",
      "Epoch: 330 | loss: 0.6917667984962463 \n",
      "Epoch: 331 | loss: 0.6917492747306824 \n",
      "Epoch: 332 | loss: 0.6917313933372498 \n",
      "Epoch: 333 | loss: 0.6917133927345276 \n",
      "Epoch: 334 | loss: 0.6916950345039368 \n",
      "Epoch: 335 | loss: 0.6916764974594116 \n",
      "Epoch: 336 | loss: 0.6916577219963074 \n",
      "Epoch: 337 | loss: 0.6916384696960449 \n",
      "Epoch: 338 | loss: 0.6916191577911377 \n",
      "Epoch: 339 | loss: 0.6915994882583618 \n",
      "Epoch: 340 | loss: 0.6915796399116516 \n",
      "Epoch: 341 | loss: 0.6915594339370728 \n",
      "Epoch: 342 | loss: 0.69153892993927 \n",
      "Epoch: 343 | loss: 0.6915181875228882 \n",
      "Epoch: 344 | loss: 0.6914970874786377 \n",
      "Epoch: 345 | loss: 0.6914757490158081 \n",
      "Epoch: 346 | loss: 0.6914541125297546 \n",
      "Epoch: 347 | loss: 0.6914321184158325 \n",
      "Epoch: 348 | loss: 0.6914098262786865 \n",
      "Epoch: 349 | loss: 0.6913873553276062 \n",
      "Epoch: 350 | loss: 0.6913642883300781 \n",
      "Epoch: 351 | loss: 0.6913411021232605 \n",
      "Epoch: 352 | loss: 0.6913174390792847 \n",
      "Epoch: 353 | loss: 0.6912935376167297 \n",
      "Epoch: 354 | loss: 0.6912692785263062 \n",
      "Epoch: 355 | loss: 0.6912446022033691 \n",
      "Epoch: 356 | loss: 0.6912196278572083 \n",
      "Epoch: 357 | loss: 0.6911942362785339 \n",
      "Epoch: 358 | loss: 0.6911685466766357 \n",
      "Epoch: 359 | loss: 0.6911424398422241 \n",
      "Epoch: 360 | loss: 0.6911159753799438 \n",
      "Epoch: 361 | loss: 0.6910890936851501 \n",
      "Epoch: 362 | loss: 0.691061794757843 \n",
      "Epoch: 363 | loss: 0.6910341382026672 \n",
      "Epoch: 364 | loss: 0.6910060048103333 \n",
      "Epoch: 365 | loss: 0.6909776329994202 \n",
      "Epoch: 366 | loss: 0.6909486651420593 \n",
      "Epoch: 367 | loss: 0.6909193396568298 \n",
      "Epoch: 368 | loss: 0.6908895969390869 \n",
      "Epoch: 369 | loss: 0.690859317779541 \n",
      "Epoch: 370 | loss: 0.6908286213874817 \n",
      "Epoch: 371 | loss: 0.6907975077629089 \n",
      "Epoch: 372 | loss: 0.690765917301178 \n",
      "Epoch: 373 | loss: 0.690733790397644 \n",
      "Epoch: 374 | loss: 0.6907013058662415 \n",
      "Epoch: 375 | loss: 0.6906681656837463 \n",
      "Epoch: 376 | loss: 0.6906346678733826 \n",
      "Epoch: 377 | loss: 0.690600574016571 \n",
      "Epoch: 378 | loss: 0.6905659437179565 \n",
      "Epoch: 379 | loss: 0.6905308365821838 \n",
      "Epoch: 380 | loss: 0.6904952526092529 \n",
      "Epoch: 381 | loss: 0.6904590129852295 \n",
      "Epoch: 382 | loss: 0.6904222369194031 \n",
      "Epoch: 383 | loss: 0.6903849244117737 \n",
      "Epoch: 384 | loss: 0.6903469562530518 \n",
      "Epoch: 385 | loss: 0.6903084516525269 \n",
      "Epoch: 386 | loss: 0.6902694702148438 \n",
      "Epoch: 387 | loss: 0.6902296543121338 \n",
      "Epoch: 388 | loss: 0.6901894211769104 \n",
      "Epoch: 389 | loss: 0.6901484727859497 \n",
      "Epoch: 390 | loss: 0.6901068687438965 \n",
      "Epoch: 391 | loss: 0.6900646090507507 \n",
      "Epoch: 392 | loss: 0.690021812915802 \n",
      "Epoch: 393 | loss: 0.6899781823158264 \n",
      "Epoch: 394 | loss: 0.6899337768554688 \n",
      "Epoch: 395 | loss: 0.6898888349533081 \n",
      "Epoch: 396 | loss: 0.6898431181907654 \n",
      "Epoch: 397 | loss: 0.6897967457771301 \n",
      "Epoch: 398 | loss: 0.6897495985031128 \n",
      "Epoch: 399 | loss: 0.6897016763687134 \n",
      "Epoch: 400 | loss: 0.6896529197692871 \n",
      "Epoch: 401 | loss: 0.6896034479141235 \n",
      "Epoch: 402 | loss: 0.6895531415939331 \n",
      "Epoch: 403 | loss: 0.6895021200180054 \n",
      "Epoch: 404 | loss: 0.6894500851631165 \n",
      "Epoch: 405 | loss: 0.6893973350524902 \n",
      "Epoch: 406 | loss: 0.6893436908721924 \n",
      "Epoch: 407 | loss: 0.6892891526222229 \n",
      "Epoch: 408 | loss: 0.6892337203025818 \n",
      "Epoch: 409 | loss: 0.6891773343086243 \n",
      "Epoch: 410 | loss: 0.6891201138496399 \n",
      "Epoch: 411 | loss: 0.6890618801116943 \n",
      "Epoch: 412 | loss: 0.6890026926994324 \n",
      "Epoch: 413 | loss: 0.688942551612854 \n",
      "Epoch: 414 | loss: 0.6888812780380249 \n",
      "Epoch: 415 | loss: 0.6888190507888794 \n",
      "Epoch: 416 | loss: 0.6887557506561279 \n",
      "Epoch: 417 | loss: 0.6886913180351257 \n",
      "Epoch: 418 | loss: 0.6886259317398071 \n",
      "Epoch: 419 | loss: 0.6885592937469482 \n",
      "Epoch: 420 | loss: 0.6884916424751282 \n",
      "Epoch: 421 | loss: 0.688422679901123 \n",
      "Epoch: 422 | loss: 0.688352644443512 \n",
      "Epoch: 423 | loss: 0.6882812976837158 \n",
      "Epoch: 424 | loss: 0.688208818435669 \n",
      "Epoch: 425 | loss: 0.6881349682807922 \n",
      "Epoch: 426 | loss: 0.6880599856376648 \n",
      "Epoch: 427 | loss: 0.6879835724830627 \n",
      "Epoch: 428 | loss: 0.6879059076309204 \n",
      "Epoch: 429 | loss: 0.6878267526626587 \n",
      "Epoch: 430 | loss: 0.6877462863922119 \n",
      "Epoch: 431 | loss: 0.6876643300056458 \n",
      "Epoch: 432 | loss: 0.687580943107605 \n",
      "Epoch: 433 | loss: 0.6874960064888 \n",
      "Epoch: 434 | loss: 0.6874096393585205 \n",
      "Epoch: 435 | loss: 0.6873217225074768 \n",
      "Epoch: 436 | loss: 0.6872320771217346 \n",
      "Epoch: 437 | loss: 0.6871408820152283 \n",
      "Epoch: 438 | loss: 0.6870481371879578 \n",
      "Epoch: 439 | loss: 0.6869534850120544 \n",
      "Epoch: 440 | loss: 0.6868572235107422 \n",
      "Epoch: 441 | loss: 0.6867591142654419 \n",
      "Epoch: 442 | loss: 0.6866592168807983 \n",
      "Epoch: 443 | loss: 0.6865574717521667 \n",
      "Epoch: 444 | loss: 0.6864539384841919 \n",
      "Epoch: 445 | loss: 0.6863482594490051 \n",
      "Epoch: 446 | loss: 0.6862406730651855 \n",
      "Epoch: 447 | loss: 0.6861311197280884 \n",
      "Epoch: 448 | loss: 0.6860195398330688 \n",
      "Epoch: 449 | loss: 0.6859056949615479 \n",
      "Epoch: 450 | loss: 0.6857898235321045 \n",
      "Epoch: 451 | loss: 0.6856716275215149 \n",
      "Epoch: 452 | loss: 0.6855511665344238 \n",
      "Epoch: 453 | loss: 0.6854283809661865 \n",
      "Epoch: 454 | loss: 0.685303270816803 \n",
      "Epoch: 455 | loss: 0.6851757168769836 \n",
      "Epoch: 456 | loss: 0.685045599937439 \n",
      "Epoch: 457 | loss: 0.6849130988121033 \n",
      "Epoch: 458 | loss: 0.6847779750823975 \n",
      "Epoch: 459 | loss: 0.6846400499343872 \n",
      "Epoch: 460 | loss: 0.6844995021820068 \n",
      "Epoch: 461 | loss: 0.684356153011322 \n",
      "Epoch: 462 | loss: 0.6842100024223328 \n",
      "Epoch: 463 | loss: 0.68406081199646 \n",
      "Epoch: 464 | loss: 0.6839087009429932 \n",
      "Epoch: 465 | loss: 0.6837536096572876 \n",
      "Epoch: 466 | loss: 0.6835953593254089 \n",
      "Epoch: 467 | loss: 0.6834338903427124 \n",
      "Epoch: 468 | loss: 0.683269202709198 \n",
      "Epoch: 469 | loss: 0.6831011772155762 \n",
      "Epoch: 470 | loss: 0.6829296946525574 \n",
      "Epoch: 471 | loss: 0.6827547550201416 \n",
      "Epoch: 472 | loss: 0.6825761198997498 \n",
      "Epoch: 473 | loss: 0.6823939681053162 \n",
      "Epoch: 474 | loss: 0.6822081208229065 \n",
      "Epoch: 475 | loss: 0.6820183396339417 \n",
      "Epoch: 476 | loss: 0.681824803352356 \n",
      "Epoch: 477 | loss: 0.681627094745636 \n",
      "Epoch: 478 | loss: 0.6814253926277161 \n",
      "Epoch: 479 | loss: 0.6812194585800171 \n",
      "Epoch: 480 | loss: 0.6810094714164734 \n",
      "Epoch: 481 | loss: 0.680794894695282 \n",
      "Epoch: 482 | loss: 0.6805760264396667 \n",
      "Epoch: 483 | loss: 0.6803524494171143 \n",
      "Epoch: 484 | loss: 0.6801243424415588 \n",
      "Epoch: 485 | loss: 0.6798915863037109 \n",
      "Epoch: 486 | loss: 0.6796538829803467 \n",
      "Epoch: 487 | loss: 0.6794112920761108 \n",
      "Epoch: 488 | loss: 0.6791636943817139 \n",
      "Epoch: 489 | loss: 0.6789108514785767 \n",
      "Epoch: 490 | loss: 0.6786528825759888 \n",
      "Epoch: 491 | loss: 0.6783894896507263 \n",
      "Epoch: 492 | loss: 0.6781207323074341 \n",
      "Epoch: 493 | loss: 0.677846372127533 \n",
      "Epoch: 494 | loss: 0.6775663495063782 \n",
      "Epoch: 495 | loss: 0.677280604839325 \n",
      "Epoch: 496 | loss: 0.676988959312439 \n",
      "Epoch: 497 | loss: 0.6766913533210754 \n",
      "Epoch: 498 | loss: 0.6763876080513 \n",
      "Epoch: 499 | loss: 0.6760777235031128 \n",
      "Epoch: 500 | loss: 0.6757614612579346 \n",
      "Epoch: 501 | loss: 0.6754389405250549 \n",
      "Epoch: 502 | loss: 0.6751096248626709 \n",
      "Epoch: 503 | loss: 0.6747738718986511 \n",
      "Epoch: 504 | loss: 0.6744312047958374 \n",
      "Epoch: 505 | loss: 0.6740818023681641 \n",
      "Epoch: 506 | loss: 0.6737253665924072 \n",
      "Epoch: 507 | loss: 0.6733618974685669 \n",
      "Epoch: 508 | loss: 0.672991156578064 \n",
      "Epoch: 509 | loss: 0.6726130843162537 \n",
      "Epoch: 510 | loss: 0.6722276210784912 \n",
      "Epoch: 511 | loss: 0.6718346476554871 \n",
      "Epoch: 512 | loss: 0.6714340448379517 \n",
      "Epoch: 513 | loss: 0.6710255146026611 \n",
      "Epoch: 514 | loss: 0.6706092953681946 \n",
      "Epoch: 515 | loss: 0.6701850295066833 \n",
      "Epoch: 516 | loss: 0.6697526574134827 \n",
      "Epoch: 517 | loss: 0.669312059879303 \n",
      "Epoch: 518 | loss: 0.6688632369041443 \n",
      "Epoch: 519 | loss: 0.6684058308601379 \n",
      "Epoch: 520 | loss: 0.6679399609565735 \n",
      "Epoch: 521 | loss: 0.6674655079841614 \n",
      "Epoch: 522 | loss: 0.6669821739196777 \n",
      "Epoch: 523 | loss: 0.6664900779724121 \n",
      "Epoch: 524 | loss: 0.6659889817237854 \n",
      "Epoch: 525 | loss: 0.6654788255691528 \n",
      "Epoch: 526 | loss: 0.6649594902992249 \n",
      "Epoch: 527 | loss: 0.6644307971000671 \n",
      "Epoch: 528 | loss: 0.6638927459716797 \n",
      "Epoch: 529 | loss: 0.6633451581001282 \n",
      "Epoch: 530 | loss: 0.6627880930900574 \n",
      "Epoch: 531 | loss: 0.6622211933135986 \n",
      "Epoch: 532 | loss: 0.6616445779800415 \n",
      "Epoch: 533 | loss: 0.6610580682754517 \n",
      "Epoch: 534 | loss: 0.6604616045951843 \n",
      "Epoch: 535 | loss: 0.65985506772995 \n",
      "Epoch: 536 | loss: 0.6592382192611694 \n",
      "Epoch: 537 | loss: 0.6586111187934875 \n",
      "Epoch: 538 | loss: 0.6579737067222595 \n",
      "Epoch: 539 | loss: 0.657325804233551 \n",
      "Epoch: 540 | loss: 0.6566674709320068 \n",
      "Epoch: 541 | loss: 0.6559984087944031 \n",
      "Epoch: 542 | loss: 0.6553186774253845 \n",
      "Epoch: 543 | loss: 0.6546281576156616 \n",
      "Epoch: 544 | loss: 0.6539267897605896 \n",
      "Epoch: 545 | loss: 0.6532145142555237 \n",
      "Epoch: 546 | loss: 0.6524911522865295 \n",
      "Epoch: 547 | loss: 0.651756763458252 \n",
      "Epoch: 548 | loss: 0.6510112285614014 \n",
      "Epoch: 549 | loss: 0.6502544283866882 \n",
      "Epoch: 550 | loss: 0.6494863033294678 \n",
      "Epoch: 551 | loss: 0.6487069725990295 \n",
      "Epoch: 552 | loss: 0.6479161977767944 \n",
      "Epoch: 553 | loss: 0.6471139788627625 \n",
      "Epoch: 554 | loss: 0.646300196647644 \n",
      "Epoch: 555 | loss: 0.6454748511314392 \n",
      "Epoch: 556 | loss: 0.6446380019187927 \n",
      "Epoch: 557 | loss: 0.6437895894050598 \n",
      "Epoch: 558 | loss: 0.6429293751716614 \n",
      "Epoch: 559 | loss: 0.642057478427887 \n",
      "Epoch: 560 | loss: 0.6411740183830261 \n",
      "Epoch: 561 | loss: 0.640278697013855 \n",
      "Epoch: 562 | loss: 0.6393716335296631 \n",
      "Epoch: 563 | loss: 0.6384527087211609 \n",
      "Epoch: 564 | loss: 0.6375221014022827 \n",
      "Epoch: 565 | loss: 0.6365795731544495 \n",
      "Epoch: 566 | loss: 0.6356253623962402 \n",
      "Epoch: 567 | loss: 0.6346592307090759 \n",
      "Epoch: 568 | loss: 0.6336813569068909 \n",
      "Epoch: 569 | loss: 0.6326916813850403 \n",
      "Epoch: 570 | loss: 0.631690263748169 \n",
      "Epoch: 571 | loss: 0.6306770443916321 \n",
      "Epoch: 572 | loss: 0.6296520829200745 \n",
      "Epoch: 573 | loss: 0.6286154389381409 \n",
      "Epoch: 574 | loss: 0.6275670528411865 \n",
      "Epoch: 575 | loss: 0.6265069842338562 \n",
      "Epoch: 576 | loss: 0.625435471534729 \n",
      "Epoch: 577 | loss: 0.6243522763252258 \n",
      "Epoch: 578 | loss: 0.6232576966285706 \n",
      "Epoch: 579 | loss: 0.6221515536308289 \n",
      "Epoch: 580 | loss: 0.6210340261459351 \n",
      "Epoch: 581 | loss: 0.6199052333831787 \n",
      "Epoch: 582 | loss: 0.6187651753425598 \n",
      "Epoch: 583 | loss: 0.6176138520240784 \n",
      "Epoch: 584 | loss: 0.6164515018463135 \n",
      "Epoch: 585 | loss: 0.6152781248092651 \n",
      "Epoch: 586 | loss: 0.6140937805175781 \n",
      "Epoch: 587 | loss: 0.6128986477851868 \n",
      "Epoch: 588 | loss: 0.6116927266120911 \n",
      "Epoch: 589 | loss: 0.6104761958122253 \n",
      "Epoch: 590 | loss: 0.6092489957809448 \n",
      "Epoch: 591 | loss: 0.6080114841461182 \n",
      "Epoch: 592 | loss: 0.6067636013031006 \n",
      "Epoch: 593 | loss: 0.6055055856704712 \n",
      "Epoch: 594 | loss: 0.6042373180389404 \n",
      "Epoch: 595 | loss: 0.6029592156410217 \n",
      "Epoch: 596 | loss: 0.6016712784767151 \n",
      "Epoch: 597 | loss: 0.6003735661506653 \n",
      "Epoch: 598 | loss: 0.5990663170814514 \n",
      "Epoch: 599 | loss: 0.597749650478363 \n",
      "Epoch: 600 | loss: 0.5964237451553345 \n",
      "Epoch: 601 | loss: 0.5950886011123657 \n",
      "Epoch: 602 | loss: 0.5937445163726807 \n",
      "Epoch: 603 | loss: 0.5923916697502136 \n",
      "Epoch: 604 | loss: 0.5910300016403198 \n",
      "Epoch: 605 | loss: 0.5896598696708679 \n",
      "Epoch: 606 | loss: 0.5882813930511475 \n",
      "Epoch: 607 | loss: 0.5868947505950928 \n",
      "Epoch: 608 | loss: 0.5855001211166382 \n",
      "Epoch: 609 | loss: 0.5840975642204285 \n",
      "Epoch: 610 | loss: 0.5826873183250427 \n",
      "Epoch: 611 | loss: 0.5812695622444153 \n",
      "Epoch: 612 | loss: 0.5798445343971252 \n",
      "Epoch: 613 | loss: 0.5784122943878174 \n",
      "Epoch: 614 | loss: 0.5769731998443604 \n",
      "Epoch: 615 | loss: 0.5755272507667542 \n",
      "Epoch: 616 | loss: 0.5740746855735779 \n",
      "Epoch: 617 | loss: 0.5726158022880554 \n",
      "Epoch: 618 | loss: 0.5711506605148315 \n",
      "Epoch: 619 | loss: 0.5696795582771301 \n",
      "Epoch: 620 | loss: 0.5682026147842407 \n",
      "Epoch: 621 | loss: 0.5667200684547424 \n",
      "Epoch: 622 | loss: 0.5652320981025696 \n",
      "Epoch: 623 | loss: 0.5637387633323669 \n",
      "Epoch: 624 | loss: 0.5622406005859375 \n",
      "Epoch: 625 | loss: 0.5607376098632812 \n",
      "Epoch: 626 | loss: 0.5592299103736877 \n",
      "Epoch: 627 | loss: 0.5577178597450256 \n",
      "Epoch: 628 | loss: 0.5562016367912292 \n",
      "Epoch: 629 | loss: 0.5546813011169434 \n",
      "Epoch: 630 | loss: 0.5531573295593262 \n",
      "Epoch: 631 | loss: 0.5516297221183777 \n",
      "Epoch: 632 | loss: 0.5500986576080322 \n",
      "Epoch: 633 | loss: 0.5485644340515137 \n",
      "Epoch: 634 | loss: 0.5470272898674011 \n",
      "Epoch: 635 | loss: 0.5454874038696289 \n",
      "Epoch: 636 | loss: 0.5439449548721313 \n",
      "Epoch: 637 | loss: 0.5424001216888428 \n",
      "Epoch: 638 | loss: 0.5408531427383423 \n",
      "Epoch: 639 | loss: 0.539304256439209 \n",
      "Epoch: 640 | loss: 0.5377535820007324 \n",
      "Epoch: 641 | loss: 0.5362014174461365 \n",
      "Epoch: 642 | loss: 0.5346479415893555 \n",
      "Epoch: 643 | loss: 0.5330933332443237 \n",
      "Epoch: 644 | loss: 0.531537652015686 \n",
      "Epoch: 645 | loss: 0.5299813747406006 \n",
      "Epoch: 646 | loss: 0.5284245014190674 \n",
      "Epoch: 647 | loss: 0.5268673896789551 \n",
      "Epoch: 648 | loss: 0.5253099799156189 \n",
      "Epoch: 649 | loss: 0.5237526297569275 \n",
      "Epoch: 650 | loss: 0.52219557762146 \n",
      "Epoch: 651 | loss: 0.5206388235092163 \n",
      "Epoch: 652 | loss: 0.51908278465271 \n",
      "Epoch: 653 | loss: 0.5175274610519409 \n",
      "Epoch: 654 | loss: 0.5159730315208435 \n",
      "Epoch: 655 | loss: 0.5144198536872864 \n",
      "Epoch: 656 | loss: 0.5128678679466248 \n",
      "Epoch: 657 | loss: 0.5113174319267273 \n",
      "Epoch: 658 | loss: 0.5097686648368835 \n",
      "Epoch: 659 | loss: 0.5082216858863831 \n",
      "Epoch: 660 | loss: 0.5066766142845154 \n",
      "Epoch: 661 | loss: 0.5051338076591492 \n",
      "Epoch: 662 | loss: 0.5035932064056396 \n",
      "Epoch: 663 | loss: 0.5020551085472107 \n",
      "Epoch: 664 | loss: 0.5005195736885071 \n",
      "Epoch: 665 | loss: 0.4989868998527527 \n",
      "Epoch: 666 | loss: 0.49745699763298035 \n",
      "Epoch: 667 | loss: 0.4959302544593811 \n",
      "Epoch: 668 | loss: 0.49440667033195496 \n",
      "Epoch: 669 | loss: 0.49288642406463623 \n",
      "Epoch: 670 | loss: 0.4913695454597473 \n",
      "Epoch: 671 | loss: 0.4898563623428345 \n",
      "Epoch: 672 | loss: 0.4883468747138977 \n",
      "Epoch: 673 | loss: 0.4868412911891937 \n",
      "Epoch: 674 | loss: 0.48533958196640015 \n",
      "Epoch: 675 | loss: 0.48384204506874084 \n",
      "Epoch: 676 | loss: 0.4823486804962158 \n",
      "Epoch: 677 | loss: 0.480859637260437 \n",
      "Epoch: 678 | loss: 0.47937506437301636 \n",
      "Epoch: 679 | loss: 0.47789496183395386 \n",
      "Epoch: 680 | loss: 0.4764195680618286 \n",
      "Epoch: 681 | loss: 0.474948912858963 \n",
      "Epoch: 682 | loss: 0.47348305583000183 \n",
      "Epoch: 683 | loss: 0.4720221757888794 \n",
      "Epoch: 684 | loss: 0.4705663025379181 \n",
      "Epoch: 685 | loss: 0.46911561489105225 \n",
      "Epoch: 686 | loss: 0.4676700532436371 \n",
      "Epoch: 687 | loss: 0.46622979640960693 \n",
      "Epoch: 688 | loss: 0.4647948741912842 \n",
      "Epoch: 689 | loss: 0.46336543560028076 \n",
      "Epoch: 690 | loss: 0.46194154024124146 \n",
      "Epoch: 691 | loss: 0.46052321791648865 \n",
      "Epoch: 692 | loss: 0.4591104984283447 \n",
      "Epoch: 693 | loss: 0.4577036201953888 \n",
      "Epoch: 694 | loss: 0.4563024640083313 \n",
      "Epoch: 695 | loss: 0.4549071490764618 \n",
      "Epoch: 696 | loss: 0.45351776480674744 \n",
      "Epoch: 697 | loss: 0.45213428139686584 \n",
      "Epoch: 698 | loss: 0.4507569372653961 \n",
      "Epoch: 699 | loss: 0.44938555359840393 \n",
      "Epoch: 700 | loss: 0.4480203688144684 \n",
      "Epoch: 701 | loss: 0.4466612935066223 \n",
      "Epoch: 702 | loss: 0.4453084468841553 \n",
      "Epoch: 703 | loss: 0.44396182894706726 \n",
      "Epoch: 704 | loss: 0.44262149930000305 \n",
      "Epoch: 705 | loss: 0.44128748774528503 \n",
      "Epoch: 706 | loss: 0.4399597942829132 \n",
      "Epoch: 707 | loss: 0.4386385381221771 \n",
      "Epoch: 708 | loss: 0.437323659658432 \n",
      "Epoch: 709 | loss: 0.4360152781009674 \n",
      "Epoch: 710 | loss: 0.43471333384513855 \n",
      "Epoch: 711 | loss: 0.4334178566932678 \n",
      "Epoch: 712 | loss: 0.43212899565696716 \n",
      "Epoch: 713 | loss: 0.43084651231765747 \n",
      "Epoch: 714 | loss: 0.4295707046985626 \n",
      "Epoch: 715 | loss: 0.4283014237880707 \n",
      "Epoch: 716 | loss: 0.4270387291908264 \n",
      "Epoch: 717 | loss: 0.42578262090682983 \n",
      "Epoch: 718 | loss: 0.42453306913375854 \n",
      "Epoch: 719 | loss: 0.4232902228832245 \n",
      "Epoch: 720 | loss: 0.4220539331436157 \n",
      "Epoch: 721 | loss: 0.4208242893218994 \n",
      "Epoch: 722 | loss: 0.4196012616157532 \n",
      "Epoch: 723 | loss: 0.41838493943214417 \n",
      "Epoch: 724 | loss: 0.41717514395713806 \n",
      "Epoch: 725 | loss: 0.4159720540046692 \n",
      "Epoch: 726 | loss: 0.414775550365448 \n",
      "Epoch: 727 | loss: 0.41358569264411926 \n",
      "Epoch: 728 | loss: 0.41240251064300537 \n",
      "Epoch: 729 | loss: 0.4112258851528168 \n",
      "Epoch: 730 | loss: 0.41005587577819824 \n",
      "Epoch: 731 | loss: 0.40889254212379456 \n",
      "Epoch: 732 | loss: 0.40773579478263855 \n",
      "Epoch: 733 | loss: 0.40658560395240784 \n",
      "Epoch: 734 | loss: 0.4054419696331024 \n",
      "Epoch: 735 | loss: 0.40430495142936707 \n",
      "Epoch: 736 | loss: 0.4031745195388794 \n",
      "Epoch: 737 | loss: 0.40205055475234985 \n",
      "Epoch: 738 | loss: 0.4009331166744232 \n",
      "Epoch: 739 | loss: 0.3998222053050995 \n",
      "Epoch: 740 | loss: 0.3987177610397339 \n",
      "Epoch: 741 | loss: 0.3976198434829712 \n",
      "Epoch: 742 | loss: 0.39652833342552185 \n",
      "Epoch: 743 | loss: 0.39544326066970825 \n",
      "Epoch: 744 | loss: 0.3943646252155304 \n",
      "Epoch: 745 | loss: 0.3932923376560211 \n",
      "Epoch: 746 | loss: 0.3922264575958252 \n",
      "Epoch: 747 | loss: 0.39116695523262024 \n",
      "Epoch: 748 | loss: 0.3901137113571167 \n",
      "Epoch: 749 | loss: 0.38906681537628174 \n",
      "Epoch: 750 | loss: 0.38802623748779297 \n",
      "Epoch: 751 | loss: 0.38699185848236084 \n",
      "Epoch: 752 | loss: 0.38596370816230774 \n",
      "Epoch: 753 | loss: 0.38494178652763367 \n",
      "Epoch: 754 | loss: 0.38392603397369385 \n",
      "Epoch: 755 | loss: 0.3829164505004883 \n",
      "Epoch: 756 | loss: 0.3819130063056946 \n",
      "Epoch: 757 | loss: 0.3809156119823456 \n",
      "Epoch: 758 | loss: 0.3799242675304413 \n",
      "Epoch: 759 | loss: 0.37893906235694885 \n",
      "Epoch: 760 | loss: 0.3779597878456116 \n",
      "Epoch: 761 | loss: 0.3769865036010742 \n",
      "Epoch: 762 | loss: 0.3760192096233368 \n",
      "Epoch: 763 | loss: 0.37505781650543213 \n",
      "Epoch: 764 | loss: 0.37410229444503784 \n",
      "Epoch: 765 | loss: 0.3731527030467987 \n",
      "Epoch: 766 | loss: 0.3722088932991028 \n",
      "Epoch: 767 | loss: 0.37127089500427246 \n",
      "Epoch: 768 | loss: 0.37033870816230774 \n",
      "Epoch: 769 | loss: 0.3694121539592743 \n",
      "Epoch: 770 | loss: 0.36849141120910645 \n",
      "Epoch: 771 | loss: 0.3675763010978699 \n",
      "Epoch: 772 | loss: 0.36666691303253174 \n",
      "Epoch: 773 | loss: 0.3657630681991577 \n",
      "Epoch: 774 | loss: 0.3648647964000702 \n",
      "Epoch: 775 | loss: 0.36397209763526917 \n",
      "Epoch: 776 | loss: 0.36308494210243225 \n",
      "Epoch: 777 | loss: 0.3622032105922699 \n",
      "Epoch: 778 | loss: 0.36132699251174927 \n",
      "Epoch: 779 | loss: 0.3604561686515808 \n",
      "Epoch: 780 | loss: 0.35959067940711975 \n",
      "Epoch: 781 | loss: 0.35873061418533325 \n",
      "Epoch: 782 | loss: 0.35787591338157654 \n",
      "Epoch: 783 | loss: 0.3570263981819153 \n",
      "Epoch: 784 | loss: 0.3561822175979614 \n",
      "Epoch: 785 | loss: 0.3553432524204254 \n",
      "Epoch: 786 | loss: 0.3545094430446625 \n",
      "Epoch: 787 | loss: 0.3536807596683502 \n",
      "Epoch: 788 | loss: 0.35285723209381104 \n",
      "Epoch: 789 | loss: 0.3520388901233673 \n",
      "Epoch: 790 | loss: 0.35122549533843994 \n",
      "Epoch: 791 | loss: 0.35041722655296326 \n",
      "Epoch: 792 | loss: 0.34961384534835815 \n",
      "Epoch: 793 | loss: 0.3488155007362366 \n",
      "Epoch: 794 | loss: 0.3480220139026642 \n",
      "Epoch: 795 | loss: 0.34723347425460815 \n",
      "Epoch: 796 | loss: 0.3464498221874237 \n",
      "Epoch: 797 | loss: 0.3456709384918213 \n",
      "Epoch: 798 | loss: 0.34489691257476807 \n",
      "Epoch: 799 | loss: 0.3441276550292969 \n",
      "Epoch: 800 | loss: 0.34336307644844055 \n",
      "Epoch: 801 | loss: 0.34260326623916626 \n",
      "Epoch: 802 | loss: 0.34184807538986206 \n",
      "Epoch: 803 | loss: 0.34109753370285034 \n",
      "Epoch: 804 | loss: 0.3403516411781311 \n",
      "Epoch: 805 | loss: 0.33961033821105957 \n",
      "Epoch: 806 | loss: 0.33887356519699097 \n",
      "Epoch: 807 | loss: 0.3381412625312805 \n",
      "Epoch: 808 | loss: 0.337413489818573 \n",
      "Epoch: 809 | loss: 0.33669009804725647 \n",
      "Epoch: 810 | loss: 0.3359711766242981 \n",
      "Epoch: 811 | loss: 0.3352566957473755 \n",
      "Epoch: 812 | loss: 0.3345465064048767 \n",
      "Epoch: 813 | loss: 0.33384063839912415 \n",
      "Epoch: 814 | loss: 0.3331391513347626 \n",
      "Epoch: 815 | loss: 0.33244186639785767 \n",
      "Epoch: 816 | loss: 0.3317488431930542 \n",
      "Epoch: 817 | loss: 0.3310600221157074 \n",
      "Epoch: 818 | loss: 0.3303753733634949 \n",
      "Epoch: 819 | loss: 0.32969486713409424 \n",
      "Epoch: 820 | loss: 0.3290184736251831 \n",
      "Epoch: 821 | loss: 0.32834622263908386 \n",
      "Epoch: 822 | loss: 0.32767799496650696 \n",
      "Epoch: 823 | loss: 0.3270137906074524 \n",
      "Epoch: 824 | loss: 0.3263535797595978 \n",
      "Epoch: 825 | loss: 0.3256973922252655 \n",
      "Epoch: 826 | loss: 0.32504507899284363 \n",
      "Epoch: 827 | loss: 0.3243967294692993 \n",
      "Epoch: 828 | loss: 0.3237522840499878 \n",
      "Epoch: 829 | loss: 0.3231116831302643 \n",
      "Epoch: 830 | loss: 0.3224748969078064 \n",
      "Epoch: 831 | loss: 0.32184192538261414 \n",
      "Epoch: 832 | loss: 0.3212127685546875 \n",
      "Epoch: 833 | loss: 0.3205873370170593 \n",
      "Epoch: 834 | loss: 0.3199656009674072 \n",
      "Epoch: 835 | loss: 0.3193475604057312 \n",
      "Epoch: 836 | loss: 0.31873324513435364 \n",
      "Epoch: 837 | loss: 0.318122535943985 \n",
      "Epoch: 838 | loss: 0.31751549243927 \n",
      "Epoch: 839 | loss: 0.3169119954109192 \n",
      "Epoch: 840 | loss: 0.3163120746612549 \n",
      "Epoch: 841 | loss: 0.31571564078330994 \n",
      "Epoch: 842 | loss: 0.3151227831840515 \n",
      "Epoch: 843 | loss: 0.31453341245651245 \n",
      "Epoch: 844 | loss: 0.31394749879837036 \n",
      "Epoch: 845 | loss: 0.3133649528026581 \n",
      "Epoch: 846 | loss: 0.31278589367866516 \n",
      "Epoch: 847 | loss: 0.31221020221710205 \n",
      "Epoch: 848 | loss: 0.31163784861564636 \n",
      "Epoch: 849 | loss: 0.31106889247894287 \n",
      "Epoch: 850 | loss: 0.31050318479537964 \n",
      "Epoch: 851 | loss: 0.30994078516960144 \n",
      "Epoch: 852 | loss: 0.3093816339969635 \n",
      "Epoch: 853 | loss: 0.3088257610797882 \n",
      "Epoch: 854 | loss: 0.3082730770111084 \n",
      "Epoch: 855 | loss: 0.30772361159324646 \n",
      "Epoch: 856 | loss: 0.30717727541923523 \n",
      "Epoch: 857 | loss: 0.3066341280937195 \n",
      "Epoch: 858 | loss: 0.30609408020973206 \n",
      "Epoch: 859 | loss: 0.30555713176727295 \n",
      "Epoch: 860 | loss: 0.30502328276634216 \n",
      "Epoch: 861 | loss: 0.3044925034046173 \n",
      "Epoch: 862 | loss: 0.3039647340774536 \n",
      "Epoch: 863 | loss: 0.3034399747848511 \n",
      "Epoch: 864 | loss: 0.3029181957244873 \n",
      "Epoch: 865 | loss: 0.3023993968963623 \n",
      "Epoch: 866 | loss: 0.3018835186958313 \n",
      "Epoch: 867 | loss: 0.3013705909252167 \n",
      "Epoch: 868 | loss: 0.30086058378219604 \n",
      "Epoch: 869 | loss: 0.30035340785980225 \n",
      "Epoch: 870 | loss: 0.29984915256500244 \n",
      "Epoch: 871 | loss: 0.2993476986885071 \n",
      "Epoch: 872 | loss: 0.29884907603263855 \n",
      "Epoch: 873 | loss: 0.29835325479507446 \n",
      "Epoch: 874 | loss: 0.29786020517349243 \n",
      "Epoch: 875 | loss: 0.2973698675632477 \n",
      "Epoch: 876 | loss: 0.29688236117362976 \n",
      "Epoch: 877 | loss: 0.29639750719070435 \n",
      "Epoch: 878 | loss: 0.2959153652191162 \n",
      "Epoch: 879 | loss: 0.29543593525886536 \n",
      "Epoch: 880 | loss: 0.29495909810066223 \n",
      "Epoch: 881 | loss: 0.2944849729537964 \n",
      "Epoch: 882 | loss: 0.2940134108066559 \n",
      "Epoch: 883 | loss: 0.2935444712638855 \n",
      "Epoch: 884 | loss: 0.29307812452316284 \n",
      "Epoch: 885 | loss: 0.2926143407821655 \n",
      "Epoch: 886 | loss: 0.29215309023857117 \n",
      "Epoch: 887 | loss: 0.29169440269470215 \n",
      "Epoch: 888 | loss: 0.2912381887435913 \n",
      "Epoch: 889 | loss: 0.29078447818756104 \n",
      "Epoch: 890 | loss: 0.29033324122428894 \n",
      "Epoch: 891 | loss: 0.28988444805145264 \n",
      "Epoch: 892 | loss: 0.2894380986690521 \n",
      "Epoch: 893 | loss: 0.2889941930770874 \n",
      "Epoch: 894 | loss: 0.2885526716709137 \n",
      "Epoch: 895 | loss: 0.2881135046482086 \n",
      "Epoch: 896 | loss: 0.28767672181129456 \n",
      "Epoch: 897 | loss: 0.2872423231601715 \n",
      "Epoch: 898 | loss: 0.2868102192878723 \n",
      "Epoch: 899 | loss: 0.28638046979904175 \n",
      "Epoch: 900 | loss: 0.28595298528671265 \n",
      "Epoch: 901 | loss: 0.2855277955532074 \n",
      "Epoch: 902 | loss: 0.285104900598526 \n",
      "Epoch: 903 | loss: 0.28468427062034607 \n",
      "Epoch: 904 | loss: 0.28426581621170044 \n",
      "Epoch: 905 | loss: 0.2838496267795563 \n",
      "Epoch: 906 | loss: 0.2834356427192688 \n",
      "Epoch: 907 | loss: 0.2830238342285156 \n",
      "Epoch: 908 | loss: 0.28261420130729675 \n",
      "Epoch: 909 | loss: 0.2822066843509674 \n",
      "Epoch: 910 | loss: 0.28180137276649475 \n",
      "Epoch: 911 | loss: 0.28139814734458923 \n",
      "Epoch: 912 | loss: 0.28099703788757324 \n",
      "Epoch: 913 | loss: 0.2805980443954468 \n",
      "Epoch: 914 | loss: 0.28020113706588745 \n",
      "Epoch: 915 | loss: 0.27980631589889526 \n",
      "Epoch: 916 | loss: 0.27941352128982544 \n",
      "Epoch: 917 | loss: 0.279022753238678 \n",
      "Epoch: 918 | loss: 0.2786340117454529 \n",
      "Epoch: 919 | loss: 0.27824732661247253 \n",
      "Epoch: 920 | loss: 0.2778625786304474 \n",
      "Epoch: 921 | loss: 0.2774798572063446 \n",
      "Epoch: 922 | loss: 0.277099072933197 \n",
      "Epoch: 923 | loss: 0.2767202854156494 \n",
      "Epoch: 924 | loss: 0.2763434052467346 \n",
      "Epoch: 925 | loss: 0.27596843242645264 \n",
      "Epoch: 926 | loss: 0.27559539675712585 \n",
      "Epoch: 927 | loss: 0.2752242982387543 \n",
      "Epoch: 928 | loss: 0.27485501766204834 \n",
      "Epoch: 929 | loss: 0.2744876444339752 \n",
      "Epoch: 930 | loss: 0.27412211894989014 \n",
      "Epoch: 931 | loss: 0.2737584710121155 \n",
      "Epoch: 932 | loss: 0.27339664101600647 \n",
      "Epoch: 933 | loss: 0.2730366587638855 \n",
      "Epoch: 934 | loss: 0.2726784944534302 \n",
      "Epoch: 935 | loss: 0.2723220884799957 \n",
      "Epoch: 936 | loss: 0.27196747064590454 \n",
      "Epoch: 937 | loss: 0.2716146409511566 \n",
      "Epoch: 938 | loss: 0.27126356959342957 \n",
      "Epoch: 939 | loss: 0.270914226770401 \n",
      "Epoch: 940 | loss: 0.2705666422843933 \n",
      "Epoch: 941 | loss: 0.2702207863330841 \n",
      "Epoch: 942 | loss: 0.269876629114151 \n",
      "Epoch: 943 | loss: 0.2695342004299164 \n",
      "Epoch: 944 | loss: 0.2691934108734131 \n",
      "Epoch: 945 | loss: 0.2688543498516083 \n",
      "Epoch: 946 | loss: 0.2685169279575348 \n",
      "Epoch: 947 | loss: 0.2681812047958374 \n",
      "Epoch: 948 | loss: 0.26784709095954895 \n",
      "Epoch: 949 | loss: 0.2675146162509918 \n",
      "Epoch: 950 | loss: 0.267183780670166 \n",
      "Epoch: 951 | loss: 0.26685452461242676 \n",
      "Epoch: 952 | loss: 0.2665269076824188 \n",
      "Epoch: 953 | loss: 0.26620087027549744 \n",
      "Epoch: 954 | loss: 0.2658763825893402 \n",
      "Epoch: 955 | loss: 0.2655535042285919 \n",
      "Epoch: 956 | loss: 0.2652321755886078 \n",
      "Epoch: 957 | loss: 0.26491236686706543 \n",
      "Epoch: 958 | loss: 0.26459410786628723 \n",
      "Epoch: 959 | loss: 0.2642773687839508 \n",
      "Epoch: 960 | loss: 0.26396217942237854 \n",
      "Epoch: 961 | loss: 0.26364845037460327 \n",
      "Epoch: 962 | loss: 0.26333627104759216 \n",
      "Epoch: 963 | loss: 0.26302558183670044 \n",
      "Epoch: 964 | loss: 0.2627163231372833 \n",
      "Epoch: 965 | loss: 0.2624085545539856 \n",
      "Epoch: 966 | loss: 0.2621022164821625 \n",
      "Epoch: 967 | loss: 0.26179736852645874 \n",
      "Epoch: 968 | loss: 0.2614939212799072 \n",
      "Epoch: 969 | loss: 0.2611919343471527 \n",
      "Epoch: 970 | loss: 0.2608913481235504 \n",
      "Epoch: 971 | loss: 0.26059216260910034 \n",
      "Epoch: 972 | loss: 0.2602944076061249 \n",
      "Epoch: 973 | loss: 0.25999805331230164 \n",
      "Epoch: 974 | loss: 0.2597030699253082 \n",
      "Epoch: 975 | loss: 0.25940942764282227 \n",
      "Epoch: 976 | loss: 0.2591172158718109 \n",
      "Epoch: 977 | loss: 0.25882628560066223 \n",
      "Epoch: 978 | loss: 0.2585367262363434 \n",
      "Epoch: 979 | loss: 0.25824853777885437 \n",
      "Epoch: 980 | loss: 0.2579616606235504 \n",
      "Epoch: 981 | loss: 0.2576761245727539 \n",
      "Epoch: 982 | loss: 0.2573918402194977 \n",
      "Epoch: 983 | loss: 0.2571089267730713 \n",
      "Epoch: 984 | loss: 0.2568272650241852 \n",
      "Epoch: 985 | loss: 0.25654691457748413 \n",
      "Epoch: 986 | loss: 0.25626784563064575 \n",
      "Epoch: 987 | loss: 0.25599005818367004 \n",
      "Epoch: 988 | loss: 0.2557135224342346 \n",
      "Epoch: 989 | loss: 0.2554382383823395 \n",
      "Epoch: 990 | loss: 0.2551642060279846 \n",
      "Epoch: 991 | loss: 0.25489142537117004 \n",
      "Epoch: 992 | loss: 0.254619836807251 \n",
      "Epoch: 993 | loss: 0.2543494701385498 \n",
      "Epoch: 994 | loss: 0.2540803551673889 \n",
      "Epoch: 995 | loss: 0.2538124918937683 \n",
      "Epoch: 996 | loss: 0.25354573130607605 \n",
      "Epoch: 997 | loss: 0.2532802224159241 \n",
      "Epoch: 998 | loss: 0.2530158758163452 \n",
      "Epoch: 999 | loss: 0.2527526915073395 \n",
      "Epoch: 1000 | loss: 0.25249072909355164 \n"
     ]
    }
   ],
   "source": [
    "# create a neural network class inheriting from the nn.Module\n",
    "# Call it NeuralNetwork and make, and use \"pass\" in the constructor\n",
    "# so that it doesn't give an error\n",
    "# Instantiate one instance of it in variable net\n",
    "\n",
    "df = pd.read_csv('data.csv', header=None)\n",
    "net = 0\n",
    "\n",
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self ):\n",
    "        super(NeuralNetwork, self).__init__()\n",
    "        self.input_layer = nn.Linear(2, 16)\n",
    "        self.hidden1 = nn.Linear(16, 8)\n",
    "        self.hidden2 = nn.Linear(8, 4)\n",
    "        self.output = nn.Linear(4, 1)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "    \n",
    "    def forward(self, x):\n",
    "        first_layer = self.input_layer(x)\n",
    "        act1 = self.sigmoid(first_layer)\n",
    "        second_layer = self.hidden1(act1)\n",
    "        act2 = self.sigmoid(second_layer)\n",
    "        thrid_layer = self.hidden2(act2)\n",
    "        act3 = self.sigmoid(thrid_layer)\n",
    "        out_layer = self.output(act3)\n",
    "        prediction = self.sigmoid(out_layer)\n",
    "        return prediction\n",
    "\n",
    "\n",
    "\n",
    "net = NeuralNetwork()\n",
    "criterion = nn.BCELoss()\n",
    "optimizer = T.optim.Adam(net.parameters(), lr=1e-3) # 0.001 == 1*10^-3\n",
    "\n",
    "X = T.from_numpy(df[[0,1]].values).float()\n",
    "y = T.from_numpy(df[[2]].values).float()\n",
    "\n",
    "x_test, y_test, x_train, y_train = train_test_split(X, y, train_size=0.7, random_state=73)\n",
    "\n",
    "epochs = 1000\n",
    "train_losses = []\n",
    "test_losses = []\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    optimizer.zero_grad() # first step: reset gradients\n",
    "\n",
    "    pred = net.forward(X) # second step: make the prediction\n",
    "\n",
    "    loss = criterion(pred, y) # third step: compute the loss\n",
    "\n",
    "    loss.backward() # 4th step: backwards pass\n",
    "\n",
    "    optimizer.step() # 5th step: saving the weights\n",
    "\n",
    "    print(f'Epoch: {epoch + 1} | loss: {loss.item()} ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "demographic-honor",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-26T07:51:28.420569Z",
     "start_time": "2021-05-26T07:51:28.412916Z"
    }
   },
   "outputs": [],
   "source": [
    "assert isinstance(net, NeuralNetwork)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "curious-syndrome",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-26T07:56:11.203531Z",
     "start_time": "2021-05-26T07:56:11.199729Z"
    }
   },
   "outputs": [],
   "source": [
    "#Rewrite the NeuralNetwork class so that the constructor receives\n",
    "# as input the input_dim and num_hidden, respectively the dimension of \n",
    "# the input and the number of hidden neurons\n",
    "# use pass again\n",
    "\n",
    "class NeuralNetwork(nn.Module):\n",
    "    pass\n",
    "    def __init__(self, input_dim, num_hidden):\n",
    "        super(NeuralNetwork).__init__()\n",
    "        pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "recreational-macro",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-26T07:56:32.252906Z",
     "start_time": "2021-05-26T07:56:32.247913Z"
    }
   },
   "outputs": [],
   "source": [
    "assert NeuralNetwork(input_dim=10, num_hidden=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bigger-inclusion",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-26T08:04:27.491588Z",
     "start_time": "2021-05-26T08:04:27.484159Z"
    }
   },
   "outputs": [],
   "source": [
    "#Rewrite the NeuralNetwork class so that the constructor receives\n",
    "# as input the input_dim, num_hidden1 and num_hidden2, respectively the dimension of \n",
    "# the input and the number of hidden neurons for the first fully connected\n",
    "# layer and the second. Define the attributes in the constructor\n",
    "# that consists of the layers, call them fc1, fc2 and fc3 and a sigmoid.\n",
    "# use pass again. Be careful to put the dimensions in the right places!\n",
    "# Since we will do a binary classification problem, fc3 will have 1 neuron\n",
    "# as output\n",
    "\n",
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self, input_dim, num_hidden1, num_hidden2):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(input_dim, num_hidden1)\n",
    "        self.fc2 = nn.Linear(num_hidden1, num_hidden2)\n",
    "        self.fc3 = nn.Linear(num_hidden2, 1)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "    \n",
    "    def forward(self, x):\n",
    "        layer1 = self.fc1(x)\n",
    "        act1 = self.sigmoid(layer1)\n",
    "        layer2 = self.fc2(act1)\n",
    "        act2 = self.sigmoid(layer2)\n",
    "        layer3 = self.fc3(act2)\n",
    "        out = self.sigmoid(layer3)\n",
    "        return out\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "hawaiian-noise",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-26T08:04:48.612004Z",
     "start_time": "2021-05-26T08:04:48.606773Z"
    }
   },
   "outputs": [],
   "source": [
    "net = NeuralNetwork(16, 16, 16)\n",
    "assert net.fc1\n",
    "assert net.fc2\n",
    "assert net.fc3\n",
    "assert net.sigmoid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "smart-southeast",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the forward pass to make a reasonable use of the attributes\n",
    "#you defined before. Follow the same reasoning we used in class\n",
    "\n",
    "model = NeuralNetwork(10, 7, 8)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "933260ee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<generator object Module.parameters at 0x7f074b2c6dd0>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "latest-sacramento",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For training a model, use the following optimizer and loss\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
    "loss = nn.BCELoss()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "lesser-bunch",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train a neural network (feel free to choose the num_hidden1 and num_hidden2)\n",
    "# on the dataset in data.csv file\n",
    "# You'll have fun with conflicting shapes and types and tensors, but\n",
    "# you'll get those errors anyway. Let's go into the wild and learn\n",
    "# by reading the errors and trying to understand them! :)\n",
    "# You can always use the provided Workbook"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
